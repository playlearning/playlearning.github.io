(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{187:function(t,e,o){},290:function(t,e,o){"use strict";var i=o(187);o.n(i).a},302:function(t,e,o){"use strict";o.r(e);o(290);var i=o(2),l=Object(i.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var t=this,e=t.$createElement,o=t._self._c||e;return o("div",{attrs:{id:"rl60days"}},[o("h3",{staticClass:"header-title"},[o("a",{staticClass:"has-text-black",attrs:{href:"https://github.com/andri27-ts/60_Days_RL_Challenge"}},[t._v("60 Days RL Challenge")])]),t._v(" "),o("p",[t._v("Learn Deep Reinforcement Learning in Depth in 60 days")]),t._v(" "),o("ul",{staticClass:"two-col"},[o("li",[o("strong",[o("a",{attrs:{href:"https://github.com/andri27-ts/60_Days_RL_Challenge#week-1---introduction"}},[t._v("Week 1 - Introduction")])])]),t._v(" "),o("ul",[o("li",[o("strong",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=JgvyzIkgxF0",rel:"nofollow"}},[t._v("An introduction to Reinforcement Learning")]),t._v(" by Arxiv Insights")])]),t._v(" "),o("li",[o("strong",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=Q4kF8sfggoI&index=1&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3",rel:"nofollow"}},[t._v("Introduction and course overview")]),t._v(" - CS294 by Levine")])]),t._v(" "),o("li",[o("strong",[o("a",{attrs:{href:"http://karpathy.github.io/2016/05/31/rl/",rel:"nofollow"}},[t._v("Deep Reinforcement Learning: Pong from Pixels")]),t._v(" by Karpathy")])])]),t._v(" "),o("li",[o("strong",[o("a",{attrs:{href:"https://github.com/andri27-ts/60_Days_RL_Challenge#week-2---rl-basics-mdp-dynamic-programming-and-model-free-control"}},[t._v("Week 2 - RL Basics")])]),t._v(" "),o("ul",[o("li",[o("strong",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=lfHX2hHRMVQ&list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&index=2",rel:"nofollow"}},[t._v("Markov Decision Process")])]),t._v(" RL by David Silver\n              "),o("ul",[o("li",[t._v("Markov Processes")]),t._v(" "),o("li",[t._v("Markov Decision Processes")])])])]),t._v(" "),o("ul",[o("li",[o("strong",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=Nd1-UUMVfz4&list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&index=3",rel:"nofollow"}},[t._v("Planning by Dynamic Programming")])]),t._v("  RL by David Silver\n              "),o("ul",[o("li",[t._v("Policy iteration")]),t._v(" "),o("li",[t._v("Value iteration")])])])]),t._v(" "),o("ul",[o("li",[o("strong",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=PnHCvfgC_ZA&index=4&list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-",rel:"nofollow"}},[t._v("Model-Free Prediction")])]),t._v("  RL by David Silver\n              "),o("ul",[o("li",[t._v("Monte Carlo Learning")]),t._v(" "),o("li",[t._v("Temporal Difference Learning")]),t._v(" "),o("li",[t._v("TD(λ)")])])])]),t._v(" "),o("ul",[o("li",[o("strong",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=0g4j2k_Ggc4&list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&index=5",rel:"nofollow"}},[t._v("Model-Free Control")])]),t._v("  RL by David Silver\n              "),o("ul",[o("li",[t._v("Ɛ-greedy policy iteration")]),t._v(" "),o("li",[t._v("GLIE Monte Carlo Search")]),t._v(" "),o("li",[t._v("SARSA")]),t._v(" "),o("li",[t._v("Importance Sampling")])])])])]),t._v(" "),o("li",[o("strong",[o("a",{attrs:{href:"https://github.com/andri27-ts/60_Days_RL_Challenge#week-3---value-function-approximation-and-dqn"}},[t._v("Week 3 - Value Function Approximation and DQN")])]),t._v(" "),o("ul",[o("li",[o("strong",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=UoPei5o4fps&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&index=6",rel:"nofollow"}},[t._v("Value functions approximation")]),t._v(" - RL by David Silver")]),t._v(" "),o("ul",[o("li",[t._v("Differentiable function approximators")]),t._v(" "),o("li",[t._v("Incremental methods")]),t._v(" "),o("li",[t._v("Batch methods (DQN)")])])])]),t._v(" "),o("ul",[o("li",[o("strong",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=nZXC5OdDfs4&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&index=7",rel:"nofollow"}},[t._v("Advanced Q-learning algorithms")]),t._v(" - DRL UC Berkley by Sergey Levine")]),t._v(" "),o("ul",[o("li",[t._v("Replay Buffer")]),t._v(" "),o("li",[t._v("Double Q-learning")]),t._v(" "),o("li",[t._v("Continous actions (NAF,DDPG)")]),t._v(" "),o("li",[t._v("Pratical tips")])])])])]),t._v(" "),o("li",[o("strong",[o("a",{attrs:{href:"https://github.com/andri27-ts/60_Days_RL_Challenge#week-4---policy-gradient-methods-and-a2c"}},[t._v("Week 4 - Policy gradient methods and A2C")])]),t._v(" "),o("ul",[o("li",[o("strong",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=KHZVXao4qXs&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&index=7",rel:"nofollow"}},[t._v("Policy gradient Methods")]),t._v(" - RL by David Silver")]),t._v(" "),o("ul",[o("li",[t._v("Finite Difference Policy Gradient")]),t._v(" "),o("li",[t._v("Monte-Carlo Policy Gradient")]),t._v(" "),o("li",[t._v("Actor-Critic Policy Gradient")])])])]),t._v(" "),o("ul",[o("li",[o("strong",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=XGmd3wcyDg8&t=0s&list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&index=3",rel:"nofollow"}},[t._v("Policy gradient intro")]),t._v(" - CS294-112 by Sergey Levine (RECAP, optional)")]),t._v(" "),o("ul",[o("li",[t._v("Policy Gradient (REINFORCE and Vanilla PG)")]),t._v(" "),o("li",[t._v("Variance reduction")])])])]),t._v(" "),o("ul",[o("li",[o("strong",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=Tol_jw5hWnI&list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&index=4",rel:"nofollow"}},[t._v("Actor-Critic")]),t._v(" - CS294-112 by Sergey Levine (More in depth)")]),t._v(" "),o("ul",[o("li",[t._v("Actor-Critic")]),t._v(" "),o("li",[t._v("Discout factor")]),t._v(" "),o("li",[t._v("Actor-Critic algorithm design (batch mode or online)")]),t._v(" "),o("li",[t._v("state-dependent baseline")])])])])]),t._v(" "),o("li",[o("strong",[o("a",{attrs:{href:"https://github.com/andri27-ts/60_Days_RL_Challenge#week-5---advanced-policy-gradients---trpo--ppo"}},[t._v("Week 5 - Advanced Policy Gradients - TRPO & PPO")])]),t._v(" "),o("ul",[o("li",[o("strong",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=ycCtmp4hcUs&t=0s&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&index=15",rel:"nofollow"}},[t._v("Advanced policy gradients")]),t._v(" - CS294-112 by Sergey Levine")]),t._v(" "),o("ul",[o("li",[t._v('Problems with "Vanilla" Policy Gradient Methods')]),t._v(" "),o("li",[t._v("Policy Performance Bounds")]),t._v(" "),o("li",[t._v("Monotonic Improvement Theory")]),t._v(" "),o("li",[t._v("Algorithms: NPO, TRPO, PPO")])])])]),t._v(" "),o("ul",[o("li",[o("strong",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=xvRrgxcpaHY",rel:"nofollow"}},[t._v("Natural Policy Gradients, TRPO, PPO")]),t._v(" - John Schulman, Berkey DRL Bootcamp - (RECAP, optional)")]),t._v(" "),o("ul",[o("li",[t._v('Limitations of "Vanilla" Policy Gradient Methods')]),t._v(" "),o("li",[t._v("Natural Policy Gradient")]),t._v(" "),o("li",[t._v("Trust Region Policy Optimization, TRPO")]),t._v(" "),o("li",[t._v("Proximal Policy Optimization, PPO")])])])])]),t._v(" "),o("li",[o("strong",[o("a",{attrs:{href:"https://github.com/andri27-ts/60_Days_RL_Challenge#week-6---evolution-strategies-and-genetic-algorithms"}},[t._v("Week 6 - Evolution Strategies and Genetic Algorithms")])]),t._v(" "),o("ul",[o("li",[o("strong",[t._v("Evolution Strategies")]),t._v(" "),o("ul",[o("li",[o("a",{attrs:{href:"http://blog.otoro.net/2017/10/29/visual-evolution-strategies/",rel:"nofollow"}},[t._v("Intro to ES: A Visual Guide to Evolution Strategies")])]),t._v(" "),o("li",[o("a",{attrs:{href:"http://blog.otoro.net/2017/11/12/evolving-stable-strategies/",rel:"nofollow"}},[t._v("ES for RL: Evolving Stable Strategies")])]),t._v(" "),o("li",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=SQtOI9jsrJ0&feature=youtu.be",rel:"nofollow"}},[t._v("Derivative-free Methods - Lecture")])]),t._v(" "),o("li",[o("a",{attrs:{href:"https://blog.openai.com/evolution-strategies/",rel:"nofollow"}},[t._v("Evolution Strategies (paper discussion)")])])])]),t._v(" "),o("li",[o("strong",[t._v("Genetic Algorithms")]),t._v(" "),o("ul",[o("li",[o("a",{attrs:{href:"https://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3",rel:"nofollow"}},[t._v("Introduction to Genetic Algorithms — Including Example Code")])])])])])]),t._v(" "),o("li",[o("strong",[o("a",{attrs:{href:"https://github.com/andri27-ts/60_Days_RL_Challenge#week-7---model-based-reinforcement-learning"}},[t._v("Week 7 - Model-Based reinforcement learning")])]),t._v(" "),o("ul",[o("li",[o("strong",[t._v("Model-Based RL by Davide Silver (Deepmind) (concise version)")]),t._v(" "),o("ul",[o("li",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=ItMutbeOHtc&index=8&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ",rel:"nofollow"}},[t._v("Integrating Learning and Planning")]),t._v(" "),o("ul",[o("li",[t._v("Model-Based RL Overview")]),t._v(" "),o("li",[t._v("Integrated architectures")]),t._v(" "),o("li",[t._v("Simulation-Based search")])])])])]),t._v(" "),o("li",[o("strong",[t._v("Model-Based RL by Sergey Levine (Berkley) (in depth version)")]),t._v(" "),o("ul",[o("li",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=yap_g0d7iBQ&index=9&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3",rel:"nofollow"}},[t._v("Learning dynamical systems from data")]),t._v(" "),o("ul",[o("li",[t._v("Overview of model-based RL")]),t._v(" "),o("li",[t._v("Global and local models")]),t._v(" "),o("li",[t._v("Learning with local models and trust regions")])])]),t._v(" "),o("li",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=AwdauFLan7M&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&index=10",rel:"nofollow"}},[t._v("Learning policies by imitating optimal controllers")]),t._v(" "),o("ul",[o("li",[t._v("Backpropagation into a policy with learned models")]),t._v(" "),o("li",[t._v("Guided policy search algorithm")]),t._v(" "),o("li",[t._v("Imitating optimal control with DAgger")])])]),t._v(" "),o("li",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=vRkIwM4GktE&index=11&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3",rel:"nofollow"}},[t._v("Advanced model learning and images")]),t._v(" "),o("ul",[o("li",[t._v("Models in latent space")]),t._v(" "),o("li",[t._v("Models directly in image space")]),t._v(" "),o("li",[t._v("Inverse models")])])])])])])]),t._v(" "),o("li",[o("strong",[o("a",{attrs:{href:"https://github.com/andri27-ts/60_Days_RL_Challenge/blob/master/README.md#week-8---advanced-concepts-and-project-of-your-choice"}},[t._v("Week 8 - Advanced Concepts and Project Of Your Choice")])]),t._v(" "),o("ul",[o("li",[t._v("Sergey Levine (Berkley)\n              "),o("ul",[o("li",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=iOYiPhu5GEk&index=13&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&t=0s",rel:"nofollow"}},[t._v("Connection between inference and control")])]),t._v(" "),o("li",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=-3BcZwgmZLk&index=14&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&t=0s",rel:"nofollow"}},[t._v("Inverse reinforcement learning")])]),t._v(" "),o("li",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=npi6B4VQ-7s&index=16&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&t=0s",rel:"nofollow"}},[t._v("Exploration (part 1)")])]),t._v(" "),o("li",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=0WbVUvKJpg4&index=17&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&t=0s",rel:"nofollow"}},[t._v("Exploration (part 2) and transfer learning")])]),t._v(" "),o("li",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=UqSx23W9RYE&index=18&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&t=0s",rel:"nofollow"}},[t._v("Multi-task learning and transfer")])]),t._v(" "),o("li",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=Xe9bktyYB34&index=18&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3",rel:"nofollow"}},[t._v("Meta-learning and parallelism")])]),t._v(" "),o("li",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=mc-DtbhhiKA&index=20&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&t=0s",rel:"nofollow"}},[t._v("Advanced imitation learning and open problems")])])])]),t._v(" "),o("li",[t._v("David Silver (Deepmind)\n              "),o("ul",[o("li",[o("a",{attrs:{href:"https://www.youtube.com/watch?v=N1LKLc6ufGY&feature=youtu.be",rel:"nofollow"}},[t._v("Classic Games")])])])])])])])])}],!1,null,"49924e7b",null);e.default=l.exports}}]);