<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Deep Reinforcement Learning Seminar | Mr.Blue&#39;s Blog</title>
    <meta name="description" content="">
    <link rel="icon" href="/imgs/shuttlecock-transparent-small.png">
  <link rel="stylesheet" href="/css/bulma.min.css">
  <link rel="stylesheet" href="/css/bulma-tooltip.min.css">
  <link rel="stylesheet" href="/css/all.min.css">
    
    <link rel="preload" href="/assets/css/0.styles.22236b47.css" as="style"><link rel="preload" href="/assets/js/app.30f4591a.js" as="script"><link rel="preload" href="/assets/js/6.31df56af.js" as="script"><link rel="preload" href="/assets/js/3.340c535d.js" as="script"><link rel="preload" href="/assets/js/19.1ffe0253.js" as="script"><link rel="preload" href="/assets/js/7.6c89a6f6.js" as="script"><link rel="preload" href="/assets/js/5.7a0fccab.js" as="script"><link rel="preload" href="/assets/js/8.4b774fed.js" as="script"><link rel="preload" href="/assets/js/9.a895b296.js" as="script"><link rel="prefetch" href="/assets/js/1.e82df926.js"><link rel="prefetch" href="/assets/js/10.9a68f9c2.js"><link rel="prefetch" href="/assets/js/11.d551b588.js"><link rel="prefetch" href="/assets/js/12.03dafeed.js"><link rel="prefetch" href="/assets/js/13.1d9f2fd6.js"><link rel="prefetch" href="/assets/js/14.f91dce36.js"><link rel="prefetch" href="/assets/js/15.a8e3f872.js"><link rel="prefetch" href="/assets/js/16.c12e73ae.js"><link rel="prefetch" href="/assets/js/17.bc94cf5a.js"><link rel="prefetch" href="/assets/js/18.4f6290fb.js"><link rel="prefetch" href="/assets/js/20.d6715a38.js"><link rel="prefetch" href="/assets/js/4.709bc619.js">
    <link rel="stylesheet" href="/assets/css/0.styles.22236b47.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><a href="/" class="home-link router-link-active"><!----> <span class="site-name">Mr.Blue's Blog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <!----></div></header> <main class="page"> <div class="content default"><h1 id="deep-reinforcement-learning-seminar"><a href="#deep-reinforcement-learning-seminar" aria-hidden="true" class="header-anchor">#</a> Deep Reinforcement Learning Seminar</h1> <h2 id="sessions"><a href="#sessions" aria-hidden="true" class="header-anchor">#</a> Sessions</h2> <div class="tip custom-block"><p><strong>Target: Learning reinforcement learning basics and advances within 10 sessions in 2 months.</strong></p></div> <table data-v-1003fdae><thead data-v-1003fdae><tr data-v-1003fdae><th data-v-1003fdae>Event</th> <th data-v-1003fdae>Date</th> <th data-v-1003fdae>Description</th> <th data-v-1003fdae>Session Materials</th></tr></thead> <tbody data-v-1003fdae></tbody></table> <h2 id="topics"><a href="#topics" aria-hidden="true" class="header-anchor">#</a> Topics</h2> <div id="" data-v-2d28cce8><div class="topic_head" data-v-2d28cce8><p class="title is-4" data-v-2d28cce8><a href="#" class="has-text-black" data-v-2d28cce8>Topic: </a></p> <p data-v-2d28cce8><span class="button is-light is-small" data-v-2d28cce8><span class="verbose-text" data-v-2d28cce8>Show</span>
      Detail
    </span> <a href="/yaml/papers/rl_nlp.yaml" target="_blank" class="button is-small is-light btn-link" data-v-2d28cce8>
      YAML  
      <i class="fas fa-cogs" data-v-2d28cce8></i></a> <a href="https://github.com/playlearning/blog/tree/master/.vuepress/public/yaml/papers" target="_blank" class="button is-small is-light btn-link" data-v-2d28cce8>
      Download Papers in List  
      <i class="fas fa-download" data-v-2d28cce8></i></a></p> <!----> <div class="field is-grouped is-grouped-multiline" data-v-2d28cce8></div> <p class="title is-5" data-v-2d28cce8>Resources</p> <ol data-v-2d28cce8></ol> <!----></div></div> <h2 id="comparison-of-different-schedules"><a href="#comparison-of-different-schedules" aria-hidden="true" class="header-anchor">#</a> Comparison of different schedules</h2> <div id="cs294-112" data-v-5d4532a3><h3 class="header-title" data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/" data-v-5d4532a3>CS294-112</a></h3> <p data-v-5d4532a3>Deep Reinforcement Learning at UC Berkeley by Sergey Levine</p> <ul resource="slides" class="lists two-col animate-3 is-small" data-v-5d4532a3><li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-1.pdf" data-v-5d4532a3>Lecture 1: Introduction and Course Overview</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-2.pdf" data-v-5d4532a3>Lecture 2: Supervised Learning and Imitation</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-3.pdf" data-v-5d4532a3>Lecture 3: TensorFlow and Neural Nets Review Session</a> <a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/misc/TF_lecture.ipynb" data-v-5d4532a3>(notebook)</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf" data-v-5d4532a3>Lecture 4: Reinforcement Learning Introduction</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf" data-v-5d4532a3>Lecture 5: Policy Gradients Introduction</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-6.pdf" data-v-5d4532a3>Lecture 6: Actor-Critic Introduction</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-7.pdf" data-v-5d4532a3>Lecture 7: Value Functions and Q-Learning</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf" data-v-5d4532a3>Lecture 8: Advanced Q-Learning Algorithms</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-9.pdf" data-v-5d4532a3>Lecture 9: Advanced Policy Gradients</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-10.pdf" data-v-5d4532a3>Lecture 10: Optimal Control and Planning</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-11.pdf" data-v-5d4532a3>Lecture 11: Model-Based Reinforcement Learning</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-12.pdf" data-v-5d4532a3>Lecture 12: Advanced Model Learning and Images</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-13.pdf" data-v-5d4532a3>Lecture 13: Learning Policies by Imitating Other Policies</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-14.pdf" data-v-5d4532a3>Lecture 14: Probability and Variational Inference Primer</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-15.pdf" data-v-5d4532a3>Lecture 15: Connection between Inference and Control</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-16.pdf" data-v-5d4532a3>Lecture 16: Inverse Reinforcement Learning</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-17.pdf" data-v-5d4532a3>Lecture 17: Exploration: Part 1</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-18.pdf" data-v-5d4532a3>Lecture 18: Exploration: Part 2</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-19.pdf" data-v-5d4532a3>Lecture 19: Transfer Learning and Multi-Task Learning</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-20.pdf" data-v-5d4532a3>Lecture 20: Meta-Learning</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-21.pdf" data-v-5d4532a3>Lecture 21: Parallelism and RL System Design</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-22.pdf" data-v-5d4532a3>Lecture 22: Advanced Imitation Learning and Open Problems</a></li> <li class="inactive" data-v-5d4532a3>Lecture 23: Guest Lecture: Craig Boutilier</li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-24.pdf" data-v-5d4532a3>Lecture 24: Guest Lecture: Gregory Kahn</a></li> <li data-v-5d4532a3><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-25.pdf" data-v-5d4532a3>Lecture 25: Guest Lecture: Quoc Le &amp; Barret Zoph</a></li> <li class="inactive" data-v-5d4532a3>Lecture 26: Guest Lecture: Karol Hausman (Canceled)</li> <li class="inactive" data-v-5d4532a3>Lecture 27: Final Project Presentations: Part 1 (No Slides)</li> <li class="inactive" data-v-5d4532a3>Lecture 28: Final Project Presentations: Part 2 (No Slides)</li></ul></div> <div id="rl60days" data-v-49924e7b><h3 class="header-title" data-v-49924e7b><a href="https://github.com/andri27-ts/60_Days_RL_Challenge" class="has-text-black" data-v-49924e7b>60 Days RL Challenge</a></h3> <p data-v-49924e7b>Learn Deep Reinforcement Learning in Depth in 60 days</p> <ul class="two-col" data-v-49924e7b><li data-v-49924e7b><strong data-v-49924e7b><a href="https://github.com/andri27-ts/60_Days_RL_Challenge#week-1---introduction" data-v-49924e7b>Week 1 - Introduction</a></strong></li> <ul data-v-49924e7b><li data-v-49924e7b><strong data-v-49924e7b><a href="https://www.youtube.com/watch?v=JgvyzIkgxF0" rel="nofollow" data-v-49924e7b>An introduction to Reinforcement Learning</a> by Arxiv Insights</strong></li> <li data-v-49924e7b><strong data-v-49924e7b><a href="https://www.youtube.com/watch?v=Q4kF8sfggoI&index=1&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3" rel="nofollow" data-v-49924e7b>Introduction and course overview</a> - CS294 by Levine</strong></li> <li data-v-49924e7b><strong data-v-49924e7b><a href="http://karpathy.github.io/2016/05/31/rl/" rel="nofollow" data-v-49924e7b>Deep Reinforcement Learning: Pong from Pixels</a> by Karpathy</strong></li></ul> <li data-v-49924e7b><strong data-v-49924e7b><a href="https://github.com/andri27-ts/60_Days_RL_Challenge#week-2---rl-basics-mdp-dynamic-programming-and-model-free-control" data-v-49924e7b>Week 2 - RL Basics</a></strong> <ul data-v-49924e7b><li data-v-49924e7b><strong data-v-49924e7b><a href="https://www.youtube.com/watch?v=lfHX2hHRMVQ&list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&index=2" rel="nofollow" data-v-49924e7b>Markov Decision Process</a></strong> RL by David Silver
              <ul data-v-49924e7b><li data-v-49924e7b>Markov Processes</li> <li data-v-49924e7b>Markov Decision Processes</li></ul></li></ul> <ul data-v-49924e7b><li data-v-49924e7b><strong data-v-49924e7b><a href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&index=3" rel="nofollow" data-v-49924e7b>Planning by Dynamic Programming</a></strong>  RL by David Silver
              <ul data-v-49924e7b><li data-v-49924e7b>Policy iteration</li> <li data-v-49924e7b>Value iteration</li></ul></li></ul> <ul data-v-49924e7b><li data-v-49924e7b><strong data-v-49924e7b><a href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&index=4&list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-" rel="nofollow" data-v-49924e7b>Model-Free Prediction</a></strong>  RL by David Silver
              <ul data-v-49924e7b><li data-v-49924e7b>Monte Carlo Learning</li> <li data-v-49924e7b>Temporal Difference Learning</li> <li data-v-49924e7b>TD(λ)</li></ul></li></ul> <ul data-v-49924e7b><li data-v-49924e7b><strong data-v-49924e7b><a href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&index=5" rel="nofollow" data-v-49924e7b>Model-Free Control</a></strong>  RL by David Silver
              <ul data-v-49924e7b><li data-v-49924e7b>Ɛ-greedy policy iteration</li> <li data-v-49924e7b>GLIE Monte Carlo Search</li> <li data-v-49924e7b>SARSA</li> <li data-v-49924e7b>Importance Sampling</li></ul></li></ul></li> <li data-v-49924e7b><strong data-v-49924e7b><a href="https://github.com/andri27-ts/60_Days_RL_Challenge#week-3---value-function-approximation-and-dqn" data-v-49924e7b>Week 3 - Value Function Approximation and DQN</a></strong> <ul data-v-49924e7b><li data-v-49924e7b><strong data-v-49924e7b><a href="https://www.youtube.com/watch?v=UoPei5o4fps&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&index=6" rel="nofollow" data-v-49924e7b>Value functions approximation</a> - RL by David Silver</strong> <ul data-v-49924e7b><li data-v-49924e7b>Differentiable function approximators</li> <li data-v-49924e7b>Incremental methods</li> <li data-v-49924e7b>Batch methods (DQN)</li></ul></li></ul> <ul data-v-49924e7b><li data-v-49924e7b><strong data-v-49924e7b><a href="https://www.youtube.com/watch?v=nZXC5OdDfs4&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&index=7" rel="nofollow" data-v-49924e7b>Advanced Q-learning algorithms</a> - DRL UC Berkley by Sergey Levine</strong> <ul data-v-49924e7b><li data-v-49924e7b>Replay Buffer</li> <li data-v-49924e7b>Double Q-learning</li> <li data-v-49924e7b>Continous actions (NAF,DDPG)</li> <li data-v-49924e7b>Pratical tips</li></ul></li></ul></li> <li data-v-49924e7b><strong data-v-49924e7b><a href="https://github.com/andri27-ts/60_Days_RL_Challenge#week-4---policy-gradient-methods-and-a2c" data-v-49924e7b>Week 4 - Policy gradient methods and A2C</a></strong> <ul data-v-49924e7b><li data-v-49924e7b><strong data-v-49924e7b><a href="https://www.youtube.com/watch?v=KHZVXao4qXs&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&index=7" rel="nofollow" data-v-49924e7b>Policy gradient Methods</a> - RL by David Silver</strong> <ul data-v-49924e7b><li data-v-49924e7b>Finite Difference Policy Gradient</li> <li data-v-49924e7b>Monte-Carlo Policy Gradient</li> <li data-v-49924e7b>Actor-Critic Policy Gradient</li></ul></li></ul> <ul data-v-49924e7b><li data-v-49924e7b><strong data-v-49924e7b><a href="https://www.youtube.com/watch?v=XGmd3wcyDg8&t=0s&list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&index=3" rel="nofollow" data-v-49924e7b>Policy gradient intro</a> - CS294-112 by Sergey Levine (RECAP, optional)</strong> <ul data-v-49924e7b><li data-v-49924e7b>Policy Gradient (REINFORCE and Vanilla PG)</li> <li data-v-49924e7b>Variance reduction</li></ul></li></ul> <ul data-v-49924e7b><li data-v-49924e7b><strong data-v-49924e7b><a href="https://www.youtube.com/watch?v=Tol_jw5hWnI&list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&index=4" rel="nofollow" data-v-49924e7b>Actor-Critic</a> - CS294-112 by Sergey Levine (More in depth)</strong> <ul data-v-49924e7b><li data-v-49924e7b>Actor-Critic</li> <li data-v-49924e7b>Discout factor</li> <li data-v-49924e7b>Actor-Critic algorithm design (batch mode or online)</li> <li data-v-49924e7b>state-dependent baseline</li></ul></li></ul></li> <li data-v-49924e7b><strong data-v-49924e7b><a href="https://github.com/andri27-ts/60_Days_RL_Challenge#week-5---advanced-policy-gradients---trpo--ppo" data-v-49924e7b>Week 5 - Advanced Policy Gradients - TRPO &amp; PPO</a></strong> <ul data-v-49924e7b><li data-v-49924e7b><strong data-v-49924e7b><a href="https://www.youtube.com/watch?v=ycCtmp4hcUs&t=0s&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&index=15" rel="nofollow" data-v-49924e7b>Advanced policy gradients</a> - CS294-112 by Sergey Levine</strong> <ul data-v-49924e7b><li data-v-49924e7b>Problems with &quot;Vanilla&quot; Policy Gradient Methods</li> <li data-v-49924e7b>Policy Performance Bounds</li> <li data-v-49924e7b>Monotonic Improvement Theory</li> <li data-v-49924e7b>Algorithms: NPO, TRPO, PPO</li></ul></li></ul> <ul data-v-49924e7b><li data-v-49924e7b><strong data-v-49924e7b><a href="https://www.youtube.com/watch?v=xvRrgxcpaHY" rel="nofollow" data-v-49924e7b>Natural Policy Gradients, TRPO, PPO</a> - John Schulman, Berkey DRL Bootcamp - (RECAP, optional)</strong> <ul data-v-49924e7b><li data-v-49924e7b>Limitations of &quot;Vanilla&quot; Policy Gradient Methods</li> <li data-v-49924e7b>Natural Policy Gradient</li> <li data-v-49924e7b>Trust Region Policy Optimization, TRPO</li> <li data-v-49924e7b>Proximal Policy Optimization, PPO</li></ul></li></ul></li> <li data-v-49924e7b><strong data-v-49924e7b><a href="https://github.com/andri27-ts/60_Days_RL_Challenge#week-6---evolution-strategies-and-genetic-algorithms" data-v-49924e7b>Week 6 - Evolution Strategies and Genetic Algorithms</a></strong> <ul data-v-49924e7b><li data-v-49924e7b><strong data-v-49924e7b>Evolution Strategies</strong> <ul data-v-49924e7b><li data-v-49924e7b><a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/" rel="nofollow" data-v-49924e7b>Intro to ES: A Visual Guide to Evolution Strategies</a></li> <li data-v-49924e7b><a href="http://blog.otoro.net/2017/11/12/evolving-stable-strategies/" rel="nofollow" data-v-49924e7b>ES for RL: Evolving Stable Strategies</a></li> <li data-v-49924e7b><a href="https://www.youtube.com/watch?v=SQtOI9jsrJ0&feature=youtu.be" rel="nofollow" data-v-49924e7b>Derivative-free Methods - Lecture</a></li> <li data-v-49924e7b><a href="https://blog.openai.com/evolution-strategies/" rel="nofollow" data-v-49924e7b>Evolution Strategies (paper discussion)</a></li></ul></li> <li data-v-49924e7b><strong data-v-49924e7b>Genetic Algorithms</strong> <ul data-v-49924e7b><li data-v-49924e7b><a href="https://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3" rel="nofollow" data-v-49924e7b>Introduction to Genetic Algorithms — Including Example Code</a></li></ul></li></ul></li> <li data-v-49924e7b><strong data-v-49924e7b><a href="https://github.com/andri27-ts/60_Days_RL_Challenge#week-7---model-based-reinforcement-learning" data-v-49924e7b>Week 7 - Model-Based reinforcement learning</a></strong> <ul data-v-49924e7b><li data-v-49924e7b><strong data-v-49924e7b>Model-Based RL by Davide Silver (Deepmind) (concise version)</strong> <ul data-v-49924e7b><li data-v-49924e7b><a href="https://www.youtube.com/watch?v=ItMutbeOHtc&index=8&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ" rel="nofollow" data-v-49924e7b>Integrating Learning and Planning</a> <ul data-v-49924e7b><li data-v-49924e7b>Model-Based RL Overview</li> <li data-v-49924e7b>Integrated architectures</li> <li data-v-49924e7b>Simulation-Based search</li></ul></li></ul></li> <li data-v-49924e7b><strong data-v-49924e7b>Model-Based RL by Sergey Levine (Berkley) (in depth version)</strong> <ul data-v-49924e7b><li data-v-49924e7b><a href="https://www.youtube.com/watch?v=yap_g0d7iBQ&index=9&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3" rel="nofollow" data-v-49924e7b>Learning dynamical systems from data</a> <ul data-v-49924e7b><li data-v-49924e7b>Overview of model-based RL</li> <li data-v-49924e7b>Global and local models</li> <li data-v-49924e7b>Learning with local models and trust regions</li></ul></li> <li data-v-49924e7b><a href="https://www.youtube.com/watch?v=AwdauFLan7M&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&index=10" rel="nofollow" data-v-49924e7b>Learning policies by imitating optimal controllers</a> <ul data-v-49924e7b><li data-v-49924e7b>Backpropagation into a policy with learned models</li> <li data-v-49924e7b>Guided policy search algorithm</li> <li data-v-49924e7b>Imitating optimal control with DAgger</li></ul></li> <li data-v-49924e7b><a href="https://www.youtube.com/watch?v=vRkIwM4GktE&index=11&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3" rel="nofollow" data-v-49924e7b>Advanced model learning and images</a> <ul data-v-49924e7b><li data-v-49924e7b>Models in latent space</li> <li data-v-49924e7b>Models directly in image space</li> <li data-v-49924e7b>Inverse models</li></ul></li></ul></li></ul></li> <li data-v-49924e7b><strong data-v-49924e7b><a href="https://github.com/andri27-ts/60_Days_RL_Challenge/blob/master/README.md#week-8---advanced-concepts-and-project-of-your-choice" data-v-49924e7b>Week 8 - Advanced Concepts and Project Of Your Choice</a></strong> <ul data-v-49924e7b><li data-v-49924e7b>Sergey Levine (Berkley)
              <ul data-v-49924e7b><li data-v-49924e7b><a href="https://www.youtube.com/watch?v=iOYiPhu5GEk&index=13&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&t=0s" rel="nofollow" data-v-49924e7b>Connection between inference and control</a></li> <li data-v-49924e7b><a href="https://www.youtube.com/watch?v=-3BcZwgmZLk&index=14&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&t=0s" rel="nofollow" data-v-49924e7b>Inverse reinforcement learning</a></li> <li data-v-49924e7b><a href="https://www.youtube.com/watch?v=npi6B4VQ-7s&index=16&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&t=0s" rel="nofollow" data-v-49924e7b>Exploration (part 1)</a></li> <li data-v-49924e7b><a href="https://www.youtube.com/watch?v=0WbVUvKJpg4&index=17&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&t=0s" rel="nofollow" data-v-49924e7b>Exploration (part 2) and transfer learning</a></li> <li data-v-49924e7b><a href="https://www.youtube.com/watch?v=UqSx23W9RYE&index=18&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&t=0s" rel="nofollow" data-v-49924e7b>Multi-task learning and transfer</a></li> <li data-v-49924e7b><a href="https://www.youtube.com/watch?v=Xe9bktyYB34&index=18&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3" rel="nofollow" data-v-49924e7b>Meta-learning and parallelism</a></li> <li data-v-49924e7b><a href="https://www.youtube.com/watch?v=mc-DtbhhiKA&index=20&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&t=0s" rel="nofollow" data-v-49924e7b>Advanced imitation learning and open problems</a></li></ul></li> <li data-v-49924e7b>David Silver (Deepmind)
              <ul data-v-49924e7b><li data-v-49924e7b><a href="https://www.youtube.com/watch?v=N1LKLc6ufGY&feature=youtu.be" rel="nofollow" data-v-49924e7b>Classic Games</a></li></ul></li></ul></li></ul></div> <h2 id="materials"><a href="#materials" aria-hidden="true" class="header-anchor">#</a> Materials</h2> <ol><li>链接
<ol><li>OpenAI Key papers list: <a href="https://spinningup.openai.com/en/latest/spinningup/keypapers.html" target="_blank" rel="noopener noreferrer">https://spinningup.openai.com/en/latest/spinningup/keypapers.html<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>A Beginner's Guide to Deep Reinforcement Learning： <a href="https://skymind.ai/wiki/deep-reinforcement-learning" target="_blank" rel="noopener noreferrer">https://skymind.ai/wiki/deep-reinforcement-learning<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>Awesome Reinforcement Learning: <a href="https://github.com/aikorea/awesome-rl" target="_blank" rel="noopener noreferrer">https://github.com/aikorea/awesome-rl<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> （2018.12.9）</li> <li><a href="https://github.com/tigerneil/awesome-deep-rl" target="_blank" rel="noopener noreferrer">https://github.com/tigerneil/awesome-deep-rl<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>Dopamine: <a href="https://github.com/google/dopamine" target="_blank" rel="noopener noreferrer">https://github.com/google/dopamine<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ol></li> <li>公开课：
<ol><li>总目录： <a href="https://github.com/kmario23/deep-learning-drizzle#balloon-reinforcement-learning-hotsprings-video_game" target="_blank" rel="noopener noreferrer">https://github.com/kmario23/deep-learning-drizzle#balloon-reinforcement-learning-hotsprings-video_game<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>Berkeley： <a href="http://rail.eecs.berkeley.edu/deeprlcourse/" target="_blank" rel="noopener noreferrer">http://rail.eecs.berkeley.edu/deeprlcourse/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>Berkeley Bootcamp： <a href="https://sites.google.com/view/deep-rl-bootcamp/lectures" target="_blank" rel="noopener noreferrer">https://sites.google.com/view/deep-rl-bootcamp/lectures<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>Standord： <a href="http://web.stanford.edu/class/cs234/index.html" target="_blank" rel="noopener noreferrer">http://web.stanford.edu/class/cs234/index.html<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>CMU： <a href="http://www.andrew.cmu.edu/course/10-703/" target="_blank" rel="noopener noreferrer">http://www.andrew.cmu.edu/course/10-703/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>David Silver： <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener noreferrer">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>OpenAI: <strong><a href="https://spinningup.openai.com/en/latest/user/introduction.html" target="_blank" rel="noopener noreferrer">https://spinningup.openai.com/en/latest/user/introduction.html<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></strong></li> <li>貌似有趣：<a href="https://simoninithomas.github.io/Deep_reinforcement_learning_Course/" target="_blank" rel="noopener noreferrer">https://simoninithomas.github.io/Deep_reinforcement_learning_Course/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ol></li> <li>教材：
<ol><li><strong>Reinforcement Learning: An Introduction, Sutton and Barto, 2018</strong> <ol><li>教材：<a href="http://incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener noreferrer">http://incompleteideas.net/book/the-book-2nd.html<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>代码：<a href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction" target="_blank" rel="noopener noreferrer">https://github.com/ShangtongZhang/reinforcement-learning-an-introduction<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ol></li> <li>Algorithms for Reinforcement Learning：<a href="https://sites.ualberta.ca/~szepesva/RLBook.html" target="_blank" rel="noopener noreferrer">https://sites.ualberta.ca/~szepesva/RLBook.html<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ol></li> <li>综述：
<ol><li>Deep Reinforcement Learning: Pong from Pixels：<a href="http://karpathy.github.io/2016/05/31/rl/" target="_blank" rel="noopener noreferrer">http://karpathy.github.io/2016/05/31/rl/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>OpenAI Spinning Up - Part 1: Key Concepts in RL：<a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#part-1-key-concepts-in-rl" target="_blank" rel="noopener noreferrer">https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#part-1-key-concepts-in-rl<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ol></li> <li>应用：
<ol><li>Robotics:
<ol><li>Stanford helicopter：<a href="http://heli.stanford.edu/" target="_blank" rel="noopener noreferrer">http://heli.stanford.edu/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>......</li></ol></li> <li>Games:
<ol><li>Atari（雅达利）：<a href="https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning/" target="_blank" rel="noopener noreferrer">https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>AlphaGo（围棋）：<a href="https://deepmind.com/research/alphago/" target="_blank" rel="noopener noreferrer">https://deepmind.com/research/alphago/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>AlphaStar（StarCraft II）：<a href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/" target="_blank" rel="noopener noreferrer">https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>OpenAI Five（Dota 2）：<a href="https://openai.com/blog/openai-five/" target="_blank" rel="noopener noreferrer">https://openai.com/blog/openai-five/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ol></li> <li>NLP:
<ol><li>Deep Reinforcement Learning for Natural Language Processing：<a href="https://sites.cs.ucsb.edu/~william/papers/ACL2018DRL4NLP.pdf" target="_blank" rel="noopener noreferrer">https://sites.cs.ucsb.edu/~william/papers/ACL2018DRL4NLP.pdf<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> <ul><li>Information Extraction</li> <li>Relational Reasoning</li> <li>Sequence Learning</li> <li>Text Classification</li> <li>Coreference Resolution</li> <li>Summarization</li> <li>Language and Vision
<ul><li>Video Captioning</li> <li>Visual-Language Navigation</li></ul></li> <li>Multi-turn dialog</li> <li>Text games：<a href="https://github.com/jvking/text-games" target="_blank" rel="noopener noreferrer">https://github.com/jvking/text-games<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ul></li> <li>News Recommendation：<a href="http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf" target="_blank" rel="noopener noreferrer">http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ol></li> <li>Others:
<ol><li>AutoML：<a href="https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html?m=1" target="_blank" rel="noopener noreferrer">https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html?m=1<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ol></li></ol></li> <li>人物
<ol><li>Richard Bellman</li> <li>Lev Pontryagin</li> <li>Andrew Barto</li> <li>Richard Sutto：<a href="http://incompleteideas.net/" target="_blank" rel="noopener noreferrer">http://incompleteideas.net/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>Pieter Abbeel：<a href="https://www2.eecs.berkeley.edu/Pubs/Faculty/abbeel.html" target="_blank" rel="noopener noreferrer">https://www2.eecs.berkeley.edu/Pubs/Faculty/abbeel.html<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ol></li> <li>最新资讯
<ol><li>Berkeley RAIL：<a href="http://rail.eecs.berkeley.edu/publications.html" target="_blank" rel="noopener noreferrer">http://rail.eecs.berkeley.edu/publications.html<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>David Silver Publications：<a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Publications.html" target="_blank" rel="noopener noreferrer">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Publications.html<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>DeepMind Blog：<a href="https://deepmind.com/blog/" target="_blank" rel="noopener noreferrer">https://deepmind.com/blog/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>OpenAI Blog：<a href="https://openai.com/blog/" target="_blank" rel="noopener noreferrer">https://openai.com/blog/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>Arxiv Sanity：<a href="http://www.arxiv-sanity.com/search?q=reinforcement+learning" target="_blank" rel="noopener noreferrer">http://www.arxiv-sanity.com/search?q=reinforcement+learning<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>University of Alberta: <a href="https://spaces.facsci.ualberta.ca/rlai/" target="_blank" rel="noopener noreferrer">https://spaces.facsci.ualberta.ca/rlai/<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ol></li></ol></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.30f4591a.js" defer></script><script src="/assets/js/6.31df56af.js" defer></script><script src="/assets/js/3.340c535d.js" defer></script><script src="/assets/js/19.1ffe0253.js" defer></script><script src="/assets/js/7.6c89a6f6.js" defer></script><script src="/assets/js/5.7a0fccab.js" defer></script><script src="/assets/js/8.4b774fed.js" defer></script><script src="/assets/js/9.a895b296.js" defer></script>
  </body>
</html>
